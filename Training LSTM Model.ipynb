{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66_Vnw4Wq5u9"
      },
      "source": [
        "## Importing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FW-4Hf3rq5vA"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "DATASET_DIR = './'\n",
        "GLOVE_DIR = './glove.6B/'\n",
        "SAVE_DIR = './'\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
        "y = X['domain1_score']\n",
        "X = X.dropna(axis=1)\n",
        "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2JDqe5Yvq5vB",
        "outputId": "bcc29f02-788c-4166-82b2-0af634d84960"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-98e1d6de-e05a-4620-a69c-e3432b367a05\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>essay_set</th>\n",
              "      <th>essay</th>\n",
              "      <th>domain1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-98e1d6de-e05a-4620-a69c-e3432b367a05')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-98e1d6de-e05a-4620-a69c-e3432b367a05 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-98e1d6de-e05a-4620-a69c-e3432b367a05');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   essay_id  essay_set                                              essay  \\\n",
              "0         1          1  Dear local newspaper, I think effects computer...   \n",
              "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
              "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
              "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
              "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
              "\n",
              "   domain1_score  \n",
              "0              8  \n",
              "1              9  \n",
              "2              7  \n",
              "3             10  \n",
              "4              8  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E_rbd6nq5vD"
      },
      "source": [
        "Minimum and Maximum Scores for each essay set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "q5cvig4Sq5vD"
      },
      "outputs": [],
      "source": [
        "minimum_scores = [-1, 2, 1, 0, 0, 0, 0, 0, 0]\n",
        "maximum_scores = [-1, 12, 6, 3, 3, 4, 4, 30, 60]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsojR9T-q5vD"
      },
      "source": [
        "## Preprocessing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZvyIqCZq5vD"
      },
      "source": [
        "We will preprocess all essays and convert them to feature vectors so that they can be fed into the RNN.\n",
        "\n",
        "These are all helper functions used to clean the essays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "SmNoVkY5q5vE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "def essay_to_wordlist(essay_v, remove_stopwords):\n",
        "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
        "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
        "    words = essay_v.lower().split()\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    return (words)\n",
        "\n",
        "def essay_to_sentences(essay_v, remove_stopwords):\n",
        "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
        "    return sentences\n",
        "\n",
        "def makeFeatureVec(words, model, num_features):\n",
        "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    num_words = 0.\n",
        "    index2word_set = set(model.wv.index_to_key)\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            num_words += 1\n",
        "            featureVec = np.add(featureVec,model.wv[word])        \n",
        "    featureVec = np.divide(featureVec,num_words)\n",
        "    return featureVec\n",
        "\n",
        "def getAvgFeatureVecs(essays, model, num_features):\n",
        "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
        "    counter = 0\n",
        "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
        "    for essay in essays:\n",
        "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
        "        counter = counter + 1\n",
        "    return essayFeatureVecs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxJqCWJlq5vF"
      },
      "source": [
        "## Defining the model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tTPf1EAq5vF"
      },
      "source": [
        "Here we define a 2-Layer LSTM Model. \n",
        "\n",
        "Note that instead of using sigmoid activation in the output layer we will use\n",
        "Relu since we are not normalising training labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "jNjXBGY2q5vF"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
        "from keras.models import Sequential, load_model, model_from_config\n",
        "import keras.backend as K\n",
        "\n",
        "def get_model():\n",
        "    \"\"\"Define the model.\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
        "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='relu'))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELWP47UUq5vG"
      },
      "source": [
        "## Training Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sfvp5t7sq5vG"
      },
      "source": [
        "Now we train the model on the dataset.\n",
        "\n",
        "We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n",
        "We will then calculate Average Kappa for all the folds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y20iiXv-q5vG",
        "outputId": "1b4c6469-68fd-4342-bbf9-c83a1c98570b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------Fold 1--------\n",
            "\n",
            "Training Word2Vec Model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-40-10d95b53e6f1>:36: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
            "  model.init_sims(replace=True)\n",
            "WARNING:gensim.models.keyedvectors:destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 1, 300)            721200    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 64)                93440     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "163/163 [==============================] - 12s 37ms/step - loss: 65.1005 - mae: 4.4290\n",
            "Epoch 2/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 40.1947 - mae: 3.6506\n",
            "Epoch 3/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 34.4161 - mae: 3.5618\n",
            "Epoch 4/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 31.8835 - mae: 3.4764\n",
            "Epoch 5/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 29.6960 - mae: 3.3290\n",
            "Epoch 6/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 27.4719 - mae: 3.1093\n",
            "Epoch 7/50\n",
            "163/163 [==============================] - 5s 32ms/step - loss: 24.5197 - mae: 2.8969\n",
            "Epoch 8/50\n",
            "163/163 [==============================] - 6s 37ms/step - loss: 21.2505 - mae: 2.6811\n",
            "Epoch 9/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 19.2098 - mae: 2.5586\n",
            "Epoch 10/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 17.4988 - mae: 2.4094\n",
            "Epoch 11/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 16.7457 - mae: 2.3354\n",
            "Epoch 12/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 15.7247 - mae: 2.2698\n",
            "Epoch 13/50\n",
            "163/163 [==============================] - 4s 25ms/step - loss: 15.3663 - mae: 2.2214\n",
            "Epoch 14/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 15.1638 - mae: 2.1987\n",
            "Epoch 15/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 15.0468 - mae: 2.1820\n",
            "Epoch 16/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 14.5536 - mae: 2.1150\n",
            "Epoch 17/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 13.6988 - mae: 2.0836\n",
            "Epoch 18/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 12.9359 - mae: 2.0393\n",
            "Epoch 19/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 13.0156 - mae: 2.0080\n",
            "Epoch 20/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 13.1318 - mae: 2.0075\n",
            "Epoch 21/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 12.7894 - mae: 1.9915\n",
            "Epoch 22/50\n",
            "163/163 [==============================] - 6s 37ms/step - loss: 12.4249 - mae: 1.9468\n",
            "Epoch 23/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 11.4521 - mae: 1.9021\n",
            "Epoch 24/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 11.1081 - mae: 1.8642\n",
            "Epoch 25/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 11.2551 - mae: 1.8791\n",
            "Epoch 26/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 11.4957 - mae: 1.8641\n",
            "Epoch 27/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 10.9535 - mae: 1.8470\n",
            "Epoch 28/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 10.7517 - mae: 1.8166\n",
            "Epoch 29/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 9.9375 - mae: 1.7912\n",
            "Epoch 30/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 10.7569 - mae: 1.8367\n",
            "Epoch 31/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 10.3640 - mae: 1.8114\n",
            "Epoch 32/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 10.2858 - mae: 1.7931\n",
            "Epoch 33/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 9.8082 - mae: 1.7556\n",
            "Epoch 34/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 10.0062 - mae: 1.7808\n",
            "Epoch 35/50\n",
            "163/163 [==============================] - 6s 36ms/step - loss: 9.9610 - mae: 1.7703\n",
            "Epoch 36/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 9.7266 - mae: 1.7573\n",
            "Epoch 37/50\n",
            "163/163 [==============================] - 4s 28ms/step - loss: 9.6985 - mae: 1.7395\n",
            "Epoch 38/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 9.5687 - mae: 1.7364\n",
            "Epoch 39/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 9.7239 - mae: 1.7327\n",
            "Epoch 40/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 9.6813 - mae: 1.7453\n",
            "Epoch 41/50\n",
            "163/163 [==============================] - 5s 31ms/step - loss: 9.1241 - mae: 1.7071\n",
            "Epoch 42/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 9.7739 - mae: 1.7476\n",
            "Epoch 43/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 9.1918 - mae: 1.6985\n",
            "Epoch 44/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 8.8712 - mae: 1.6822\n",
            "Epoch 45/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 9.1500 - mae: 1.7069\n",
            "Epoch 46/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 8.7194 - mae: 1.6757\n",
            "Epoch 47/50\n",
            "163/163 [==============================] - 5s 31ms/step - loss: 9.2398 - mae: 1.6924\n",
            "Epoch 48/50\n",
            "163/163 [==============================] - 6s 36ms/step - loss: 9.0688 - mae: 1.7011\n",
            "Epoch 49/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 8.6863 - mae: 1.6583\n",
            "Epoch 50/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 8.6258 - mae: 1.6494\n",
            "82/82 [==============================] - 1s 6ms/step\n",
            "Kappa Score: 0.9632765547247935\n",
            "\n",
            "--------Fold 2--------\n",
            "\n",
            "Training Word2Vec Model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-40-10d95b53e6f1>:36: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
            "  model.init_sims(replace=True)\n",
            "WARNING:gensim.models.keyedvectors:destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 1, 300)            721200    \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 64)                93440     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "163/163 [==============================] - 11s 26ms/step - loss: 69.2932 - mae: 4.5166\n",
            "Epoch 2/50\n",
            "163/163 [==============================] - 5s 32ms/step - loss: 41.4867 - mae: 3.6782\n",
            "Epoch 3/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 34.4678 - mae: 3.5534\n",
            "Epoch 4/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 30.5733 - mae: 3.4199\n",
            "Epoch 5/50\n",
            "163/163 [==============================] - 6s 36ms/step - loss: 29.0305 - mae: 3.2646\n",
            "Epoch 6/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 26.9205 - mae: 3.0572\n",
            "Epoch 7/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 24.0519 - mae: 2.8366\n",
            "Epoch 8/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 19.7725 - mae: 2.5670\n",
            "Epoch 9/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 18.5606 - mae: 2.4417\n",
            "Epoch 10/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 16.9369 - mae: 2.3721\n",
            "Epoch 11/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 16.3703 - mae: 2.3116\n",
            "Epoch 12/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 15.3021 - mae: 2.2277\n",
            "Epoch 13/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 14.9468 - mae: 2.1805\n",
            "Epoch 14/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 13.7079 - mae: 2.1073\n",
            "Epoch 15/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 13.9502 - mae: 2.0992\n",
            "Epoch 16/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 12.5901 - mae: 2.0198\n",
            "Epoch 17/50\n",
            "163/163 [==============================] - 5s 31ms/step - loss: 12.8241 - mae: 2.0170\n",
            "Epoch 18/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 12.6878 - mae: 1.9994\n",
            "Epoch 19/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 12.5069 - mae: 1.9682\n",
            "Epoch 20/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 11.7364 - mae: 1.9372\n",
            "Epoch 21/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 11.3886 - mae: 1.9047\n",
            "Epoch 22/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 11.3322 - mae: 1.8742\n",
            "Epoch 23/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 11.2218 - mae: 1.8716\n",
            "Epoch 24/50\n",
            "163/163 [==============================] - 5s 31ms/step - loss: 11.6690 - mae: 1.8828\n",
            "Epoch 25/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 10.9013 - mae: 1.8355\n",
            "Epoch 26/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 10.4031 - mae: 1.8180\n",
            "Epoch 27/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 10.5235 - mae: 1.8070\n",
            "Epoch 28/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 10.4176 - mae: 1.8008\n",
            "Epoch 29/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 10.2290 - mae: 1.7835\n",
            "Epoch 30/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 10.5047 - mae: 1.7984\n",
            "Epoch 31/50\n",
            "163/163 [==============================] - 6s 38ms/step - loss: 9.8747 - mae: 1.7563\n",
            "Epoch 32/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 9.4090 - mae: 1.7391\n",
            "Epoch 33/50\n",
            "163/163 [==============================] - 5s 34ms/step - loss: 9.8336 - mae: 1.7559\n",
            "Epoch 34/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 9.4970 - mae: 1.7425\n",
            "Epoch 35/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 9.6822 - mae: 1.7418\n",
            "Epoch 36/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 9.4863 - mae: 1.7350\n",
            "Epoch 37/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 9.3723 - mae: 1.7224\n",
            "Epoch 38/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 9.5833 - mae: 1.7311\n",
            "Epoch 39/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 8.9085 - mae: 1.6924\n",
            "Epoch 40/50\n",
            "163/163 [==============================] - 4s 25ms/step - loss: 9.2428 - mae: 1.6978\n",
            "Epoch 41/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 8.8253 - mae: 1.6853\n",
            "Epoch 42/50\n",
            "163/163 [==============================] - 4s 28ms/step - loss: 8.5089 - mae: 1.6640\n",
            "Epoch 43/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 9.1272 - mae: 1.7038\n",
            "Epoch 44/50\n",
            "163/163 [==============================] - 6s 38ms/step - loss: 9.0584 - mae: 1.6753\n",
            "Epoch 45/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 9.0310 - mae: 1.6785\n",
            "Epoch 46/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 8.8035 - mae: 1.6696\n",
            "Epoch 47/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 8.8232 - mae: 1.6716\n",
            "Epoch 48/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 8.4236 - mae: 1.6528\n",
            "Epoch 49/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 8.5993 - mae: 1.6547\n",
            "Epoch 50/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 8.2439 - mae: 1.6314\n",
            "82/82 [==============================] - 1s 6ms/step\n",
            "Kappa Score: 0.9546439876902368\n",
            "\n",
            "--------Fold 3--------\n",
            "\n",
            "Training Word2Vec Model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-40-10d95b53e6f1>:36: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
            "  model.init_sims(replace=True)\n",
            "WARNING:gensim.models.keyedvectors:destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_4 (LSTM)               (None, 1, 300)            721200    \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 64)                93440     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "163/163 [==============================] - 10s 33ms/step - loss: 65.3882 - mae: 4.4096\n",
            "Epoch 2/50\n",
            "163/163 [==============================] - 5s 31ms/step - loss: 40.4763 - mae: 3.6237\n",
            "Epoch 3/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 34.4823 - mae: 3.4943\n",
            "Epoch 4/50\n",
            "163/163 [==============================] - 5s 32ms/step - loss: 31.5459 - mae: 3.4285\n",
            "Epoch 5/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 29.3658 - mae: 3.2218\n",
            "Epoch 6/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 27.7309 - mae: 3.0800\n",
            "Epoch 7/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 24.5686 - mae: 2.8899\n",
            "Epoch 8/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 20.9978 - mae: 2.6492\n",
            "Epoch 9/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 18.9503 - mae: 2.4998\n",
            "Epoch 10/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 16.5726 - mae: 2.3532\n",
            "Epoch 11/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 15.6696 - mae: 2.2836\n",
            "Epoch 12/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 15.1055 - mae: 2.2234\n",
            "Epoch 13/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 14.6264 - mae: 2.1803\n",
            "Epoch 14/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 14.0139 - mae: 2.1341\n",
            "Epoch 15/50\n",
            "163/163 [==============================] - 5s 31ms/step - loss: 12.9270 - mae: 2.0588\n",
            "Epoch 16/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 12.9032 - mae: 2.0430\n",
            "Epoch 17/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 12.9401 - mae: 2.0250\n",
            "Epoch 18/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 12.3113 - mae: 1.9814\n",
            "Epoch 19/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 12.3887 - mae: 1.9649\n",
            "Epoch 20/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 12.1755 - mae: 1.9334\n",
            "Epoch 21/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 11.0104 - mae: 1.8803\n",
            "Epoch 22/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 11.1104 - mae: 1.8640\n",
            "Epoch 23/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 10.9883 - mae: 1.8537\n",
            "Epoch 24/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 10.7961 - mae: 1.8355\n",
            "Epoch 25/50\n",
            "163/163 [==============================] - 5s 27ms/step - loss: 10.4457 - mae: 1.8081\n",
            "Epoch 26/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 10.4844 - mae: 1.7983\n",
            "Epoch 27/50\n",
            "163/163 [==============================] - 6s 36ms/step - loss: 10.7405 - mae: 1.7894\n",
            "Epoch 28/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 10.0078 - mae: 1.7730\n",
            "Epoch 29/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 10.6010 - mae: 1.7784\n",
            "Epoch 30/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 9.7306 - mae: 1.7529\n",
            "Epoch 31/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 9.6228 - mae: 1.7396\n",
            "Epoch 32/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 9.5584 - mae: 1.7298\n",
            "Epoch 33/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 9.4987 - mae: 1.7243\n",
            "Epoch 34/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 9.1679 - mae: 1.7042\n",
            "Epoch 35/50\n",
            "163/163 [==============================] - 5s 34ms/step - loss: 9.3590 - mae: 1.7118\n",
            "Epoch 36/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 9.4783 - mae: 1.7029\n",
            "Epoch 37/50\n",
            "163/163 [==============================] - 5s 32ms/step - loss: 9.3279 - mae: 1.6948\n",
            "Epoch 38/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 9.1417 - mae: 1.6922\n",
            "Epoch 39/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 8.8315 - mae: 1.6707\n",
            "Epoch 40/50\n",
            "163/163 [==============================] - 6s 39ms/step - loss: 8.8450 - mae: 1.6823\n",
            "Epoch 41/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 8.5464 - mae: 1.6485\n",
            "Epoch 42/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 8.6231 - mae: 1.6505\n",
            "Epoch 43/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 8.4520 - mae: 1.6440\n",
            "Epoch 44/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 8.6703 - mae: 1.6596\n",
            "Epoch 45/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 8.5272 - mae: 1.6631\n",
            "Epoch 46/50\n",
            "163/163 [==============================] - 5s 32ms/step - loss: 8.6244 - mae: 1.6526\n",
            "Epoch 47/50\n",
            "163/163 [==============================] - 5s 31ms/step - loss: 8.8965 - mae: 1.6655\n",
            "Epoch 48/50\n",
            "163/163 [==============================] - 5s 34ms/step - loss: 8.7667 - mae: 1.6509\n",
            "Epoch 49/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 8.3975 - mae: 1.6299\n",
            "Epoch 50/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 8.1244 - mae: 1.6289\n",
            "82/82 [==============================] - 1s 6ms/step\n",
            "Kappa Score: 0.9531941253651172\n",
            "\n",
            "--------Fold 4--------\n",
            "\n",
            "Training Word2Vec Model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-40-10d95b53e6f1>:36: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
            "  model.init_sims(replace=True)\n",
            "WARNING:gensim.models.keyedvectors:destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_6 (LSTM)               (None, 1, 300)            721200    \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 64)                93440     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "163/163 [==============================] - 10s 28ms/step - loss: 64.4299 - mae: 4.3831\n",
            "Epoch 2/50\n",
            "163/163 [==============================] - 6s 38ms/step - loss: 39.1191 - mae: 3.5804\n",
            "Epoch 3/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 32.9264 - mae: 3.4691\n",
            "Epoch 4/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 29.8098 - mae: 3.3757\n",
            "Epoch 5/50\n",
            "163/163 [==============================] - 5s 34ms/step - loss: 28.2191 - mae: 3.2265\n",
            "Epoch 6/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 26.3696 - mae: 3.0329\n",
            "Epoch 7/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 23.5490 - mae: 2.8298\n",
            "Epoch 8/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 20.6455 - mae: 2.6225\n",
            "Epoch 9/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 18.6661 - mae: 2.4884\n",
            "Epoch 10/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 16.7878 - mae: 2.3552\n",
            "Epoch 11/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 16.5585 - mae: 2.3247\n",
            "Epoch 12/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 14.9126 - mae: 2.2209\n",
            "Epoch 13/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 14.5851 - mae: 2.1727\n",
            "Epoch 14/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 14.5686 - mae: 2.1538\n",
            "Epoch 15/50\n",
            "163/163 [==============================] - 6s 38ms/step - loss: 13.5195 - mae: 2.0992\n",
            "Epoch 16/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 13.5952 - mae: 2.0708\n",
            "Epoch 17/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 12.6929 - mae: 2.0163\n",
            "Epoch 18/50\n",
            "163/163 [==============================] - 5s 34ms/step - loss: 11.7789 - mae: 1.9704\n",
            "Epoch 19/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 12.2239 - mae: 1.9729\n",
            "Epoch 20/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 11.4295 - mae: 1.9169\n",
            "Epoch 21/50\n",
            "163/163 [==============================] - 5s 32ms/step - loss: 11.5848 - mae: 1.9081\n",
            "Epoch 22/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 11.3838 - mae: 1.8878\n",
            "Epoch 23/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 10.6472 - mae: 1.8495\n",
            "Epoch 24/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 10.9882 - mae: 1.8572\n",
            "Epoch 25/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 10.3449 - mae: 1.8074\n",
            "Epoch 26/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 10.1788 - mae: 1.7912\n",
            "Epoch 27/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 10.0006 - mae: 1.7658\n",
            "Epoch 28/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 10.0425 - mae: 1.7748\n",
            "Epoch 29/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 9.7441 - mae: 1.7532\n",
            "Epoch 30/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 9.6927 - mae: 1.7469\n",
            "Epoch 31/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 8.9877 - mae: 1.7073\n",
            "Epoch 32/50\n",
            "163/163 [==============================] - 4s 25ms/step - loss: 9.7425 - mae: 1.7496\n",
            "Epoch 33/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 9.7218 - mae: 1.7365\n",
            "Epoch 34/50\n",
            "163/163 [==============================] - 6s 37ms/step - loss: 8.9846 - mae: 1.7141\n",
            "Epoch 35/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 9.5721 - mae: 1.7369\n",
            "Epoch 36/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 9.2726 - mae: 1.7239\n",
            "Epoch 37/50\n",
            "163/163 [==============================] - 5s 31ms/step - loss: 9.0160 - mae: 1.6781\n",
            "Epoch 38/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 8.7897 - mae: 1.6710\n",
            "Epoch 39/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 8.5082 - mae: 1.6548\n",
            "Epoch 40/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 8.8586 - mae: 1.6664\n",
            "Epoch 41/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 8.5737 - mae: 1.6526\n",
            "Epoch 42/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 8.7310 - mae: 1.6582\n",
            "Epoch 43/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 8.3737 - mae: 1.6434\n",
            "Epoch 44/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 8.5909 - mae: 1.6378\n",
            "Epoch 45/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 8.4802 - mae: 1.6388\n",
            "Epoch 46/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 8.5362 - mae: 1.6295\n",
            "Epoch 47/50\n",
            "163/163 [==============================] - 6s 36ms/step - loss: 8.4100 - mae: 1.6143\n",
            "Epoch 48/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 8.5172 - mae: 1.6308\n",
            "Epoch 49/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 8.5661 - mae: 1.6333\n",
            "Epoch 50/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 8.0868 - mae: 1.6070\n",
            "82/82 [==============================] - 1s 6ms/step\n",
            "Kappa Score: 0.9635487289899051\n",
            "\n",
            "--------Fold 5--------\n",
            "\n",
            "Training Word2Vec Model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-40-10d95b53e6f1>:36: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
            "  model.init_sims(replace=True)\n",
            "WARNING:gensim.models.keyedvectors:destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_8 (LSTM)               (None, 1, 300)            721200    \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 64)                93440     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "163/163 [==============================] - 9s 28ms/step - loss: 67.6527 - mae: 4.4740\n",
            "Epoch 2/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 40.3311 - mae: 3.6063\n",
            "Epoch 3/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 34.1272 - mae: 3.5355\n",
            "Epoch 4/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 31.3819 - mae: 3.4324\n",
            "Epoch 5/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 29.4038 - mae: 3.2941\n",
            "Epoch 6/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 27.3099 - mae: 3.1007\n",
            "Epoch 7/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 25.2288 - mae: 2.9075\n",
            "Epoch 8/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 21.5881 - mae: 2.6865\n",
            "Epoch 9/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 18.9302 - mae: 2.5209\n",
            "Epoch 10/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 18.2538 - mae: 2.4450\n",
            "Epoch 11/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 16.0958 - mae: 2.3039\n",
            "Epoch 12/50\n",
            "163/163 [==============================] - 6s 40ms/step - loss: 15.3946 - mae: 2.2654\n",
            "Epoch 13/50\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 14.4091 - mae: 2.1770\n",
            "Epoch 14/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 14.0728 - mae: 2.1601\n",
            "Epoch 15/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 13.5333 - mae: 2.0911\n",
            "Epoch 16/50\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 12.3422 - mae: 2.0033\n",
            "Epoch 17/50\n",
            "163/163 [==============================] - 5s 31ms/step - loss: 12.7005 - mae: 2.0182\n",
            "Epoch 18/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 12.2293 - mae: 1.9863\n",
            "Epoch 19/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 11.8519 - mae: 1.9383\n",
            "Epoch 20/50\n",
            "163/163 [==============================] - 5s 34ms/step - loss: 11.7967 - mae: 1.9198\n",
            "Epoch 21/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 11.3698 - mae: 1.9052\n",
            "Epoch 22/50\n",
            "163/163 [==============================] - 4s 25ms/step - loss: 11.1342 - mae: 1.8783\n",
            "Epoch 23/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 11.0767 - mae: 1.8678\n",
            "Epoch 24/50\n",
            "163/163 [==============================] - 4s 28ms/step - loss: 11.1562 - mae: 1.8631\n",
            "Epoch 25/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 10.6222 - mae: 1.8296\n",
            "Epoch 26/50\n",
            "163/163 [==============================] - 5s 32ms/step - loss: 10.3259 - mae: 1.8129\n",
            "Epoch 27/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 10.4326 - mae: 1.8181\n",
            "Epoch 28/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 10.0731 - mae: 1.7900\n",
            "Epoch 29/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 10.1192 - mae: 1.7827\n",
            "Epoch 30/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 10.1197 - mae: 1.7865\n",
            "Epoch 31/50\n",
            "163/163 [==============================] - 6s 37ms/step - loss: 10.0690 - mae: 1.7794\n",
            "Epoch 32/50\n",
            "163/163 [==============================] - 5s 32ms/step - loss: 9.9099 - mae: 1.7455\n",
            "Epoch 33/50\n",
            "163/163 [==============================] - 6s 36ms/step - loss: 9.9265 - mae: 1.7629\n",
            "Epoch 34/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 10.1729 - mae: 1.7670\n",
            "Epoch 35/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 9.7624 - mae: 1.7435\n",
            "Epoch 36/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 9.4742 - mae: 1.7201\n",
            "Epoch 37/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 9.0421 - mae: 1.6932\n",
            "Epoch 38/50\n",
            "163/163 [==============================] - 6s 37ms/step - loss: 9.6300 - mae: 1.7264\n",
            "Epoch 39/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 9.3004 - mae: 1.7089\n",
            "Epoch 40/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 9.1648 - mae: 1.7037\n",
            "Epoch 41/50\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 9.5192 - mae: 1.7167\n",
            "Epoch 42/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 8.9374 - mae: 1.6742\n",
            "Epoch 43/50\n",
            "163/163 [==============================] - 5s 30ms/step - loss: 9.0704 - mae: 1.6860\n",
            "Epoch 44/50\n",
            "163/163 [==============================] - 6s 36ms/step - loss: 9.1716 - mae: 1.6911\n",
            "Epoch 45/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 8.8704 - mae: 1.6646\n",
            "Epoch 46/50\n",
            "163/163 [==============================] - 6s 35ms/step - loss: 8.9574 - mae: 1.6709\n",
            "Epoch 47/50\n",
            "163/163 [==============================] - 4s 28ms/step - loss: 8.4620 - mae: 1.6459\n",
            "Epoch 48/50\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 8.3413 - mae: 1.6362\n",
            "Epoch 49/50\n",
            "163/163 [==============================] - 6s 34ms/step - loss: 8.5034 - mae: 1.6396\n",
            "Epoch 50/50\n",
            "163/163 [==============================] - 5s 28ms/step - loss: 8.2640 - mae: 1.6219\n",
            "82/82 [==============================] - 1s 6ms/step\n",
            "Kappa Score: 0.9599085824590857\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True)\n",
        "cv_data = cv.split(X)\n",
        "results = []\n",
        "y_pred_list = []\n",
        "\n",
        "count = 1\n",
        "for traincv, testcv in cv_data:\n",
        "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
        "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
        "    \n",
        "    train_essays = X_train['essay']\n",
        "    test_essays = X_test['essay']\n",
        "    \n",
        "    sentences = []\n",
        "    \n",
        "    for essay in train_essays:\n",
        "            # Obtaining all sentences from the training essays.\n",
        "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
        "            \n",
        "    # Initializing variables for word2vec model.\n",
        "    num_features = 300 \n",
        "    min_word_count = 40\n",
        "    num_workers = 4\n",
        "    context = 10\n",
        "    downsampling = 1e-3\n",
        "\n",
        "    print(\"Training Word2Vec Model...\")\n",
        "    model = Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
        "\n",
        "    model.init_sims(replace=True)\n",
        "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
        "\n",
        "    clean_train_essays = []\n",
        "    \n",
        "    # Generate training and testing data word vectors.\n",
        "    for essay_v in train_essays:\n",
        "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
        "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
        "    \n",
        "    clean_test_essays = []\n",
        "    for essay_v in test_essays:\n",
        "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
        "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
        "    \n",
        "    trainDataVecs = np.array(trainDataVecs)\n",
        "    testDataVecs = np.array(testDataVecs)\n",
        "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
        "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
        "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
        "    \n",
        "    lstm_model = get_model()\n",
        "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
        "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
        "    y_pred = lstm_model.predict(testDataVecs)\n",
        "    \n",
        "    # Save any one of the 8 models.\n",
        "    if count == 5:\n",
        "         lstm_model.save('./final_lstm.h5')\n",
        "    \n",
        "    # Round y_pred to the nearest integer.\n",
        "    y_pred = np.around(y_pred)\n",
        "    \n",
        "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
        "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
        "    print(\"Kappa Score: {}\".format(result))\n",
        "    results.append(result)\n",
        "\n",
        "    count += 1\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D4zybKlq5vH",
        "outputId": "9e636a68-7280-49e9-97fe-82f38c2a69f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Kappa score after a 5-fold cross validation:  0.9589\n"
          ]
        }
      ],
      "source": [
        "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "pydbWjNUq5vH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
